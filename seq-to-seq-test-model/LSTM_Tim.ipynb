{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eng -> german sentence pairs\n",
    "pairs = [\n",
    "    [\"i am a student\", \"ich bin ein student\"],\n",
    "    [\"he is a teacher\", \"er ist ein lehrer\"],\n",
    "    [\"she is happy\", \"sie ist glücklich\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence:str)->(list):\n",
    "    return sentence.lower().split() \n",
    "\n",
    "def build_vocab(sentences:list)->(dict):\n",
    "    # building a vocabullary so each word had an index\n",
    "    # <pad> - to align sentences\n",
    "    # <sos> - \"start of sentence\" (inserted before each sentence)\n",
    "    # <eos> - \"end of sentence\" (inserted at the end)\n",
    "    vocab = {\"<pad>\":0, \"<sos>\":1, \"<eos>\":2,}\n",
    "    idx = 3 # starting point\n",
    "    for sentence in sentences:\n",
    "        for word in tokenize(sentence):\n",
    "            if word not in vocab:\n",
    "                vocab[word] = idx\n",
    "                idx+=1\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, '<sos>': 1, '<eos>': 2, 'i': 3, 'am': 4, 'a': 5, 'student': 6, 'he': 7, 'is': 8, 'teacher': 9, 'she': 10, 'happy': 11}\n",
      "{'<pad>': 0, '<sos>': 1, '<eos>': 2, 'ich': 3, 'bin': 4, 'ein': 5, 'student': 6, 'er': 7, 'ist': 8, 'lehrer': 9, 'sie': 10, 'glücklich': 11}\n"
     ]
    }
   ],
   "source": [
    "# creating eng and ger vocabs\n",
    "eng_vocab = build_vocab([n[0] for n in pairs])\n",
    "german_vocab = build_vocab([n[1] for n in pairs])\n",
    "print(eng_vocab)\n",
    "print(german_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 3, 4, 5, 6, 2], [1, 7, 8, 5, 9, 2], [1, 10, 8, 11, 2]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sentence_to_indices(sentence:str, vocab:dict)->(list):\n",
    "    tokens = tokenize(sentence)\n",
    "    return [vocab[\"<sos>\"]]+[vocab[n] for n in tokens]+[vocab[\"<eos>\"]]\n",
    "\n",
    "eng_indices = [sentence_to_indices(n[0], eng_vocab) for n in pairs]\n",
    "eng_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch(pairs:list, eng_vocab:dict, german_vocab:dict):\n",
    "    src_batch = [] # eng\n",
    "    trg_batch = [] #german\n",
    "\n",
    "    for eng, ger in pairs:\n",
    "        src = sentence_to_indices(eng, eng_vocab)\n",
    "        trg = sentence_to_indices(ger, german_vocab)\n",
    "\n",
    "        # torch.tensor(src, dtype=torch.long) -> converting to the tensors (arrays)\n",
    "        src_batch.append(torch.tensor(src, dtype=torch.long))\n",
    "        trg_batch.append(torch.tensor(trg, dtype=torch.long))\n",
    "\n",
    "    # src_batch = [\n",
    "        # tensor([1, 3, 4, 2]),        # \"i am happy\"\n",
    "        # tensor([1, 3, 4, 5, 6, 2])   # \"i am a student\"\n",
    "        # ]\n",
    "    # pad_sequence(src_batch, padding_value=0) →\n",
    "        # tensor([\n",
    "        # [1, 1],\n",
    "        # [3, 3],\n",
    "        # [4, 4],\n",
    "        # [2, 5],\n",
    "        # [0, 6],\n",
    "        # [0, 2]\n",
    "        # ])  so, the first column is the first sentence and the second is the second the zeros are the paddings, so we had vectors of the same lengths\n",
    "\n",
    "    src_batch = nn.utils.rnn.pad_sequence(src_batch, padding_value=0)\n",
    "    trg_batch = nn.utils.rnn.pad_sequence(trg_batch, padding_value=0)\n",
    "\n",
    "    return src_batch, trg_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  1,  1],\n",
       "        [ 3,  7, 10],\n",
       "        [ 4,  8,  8],\n",
       "        [ 5,  5, 11],\n",
       "        [ 6,  9,  2],\n",
       "        [ 2,  2,  0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_batch, trg_batch = prepare_batch(pairs, eng_vocab, german_vocab)\n",
    "# getting the batches and send them to the device\n",
    "src_batch=src_batch.to(device)\n",
    "trg_batch=trg_batch.to(device)\n",
    "src_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        # input_dim - how many words are in the dictionary (dictionary size)\n",
    "        # emb_dim - size of each embedding (vector length, e.g. 32 or 100)\n",
    "        # hidden_dim - how many neurons inside the LSTM, the size of the \"memory\"\n",
    "\n",
    "        # Example:\n",
    "        # The word \"student\" has an embedding index of 42\n",
    "        # embedding(42) produces a vector of sorts: [0.12, -0.03, 0.44, ..., 0.08] of length emb_dim\n",
    "\n",
    "        # Input (indices):       [1, 4, 5, 2]\n",
    "        # → Embedding Layer →    [[...], [...], [...], [...]]  # (seq_len, emb_dim)\n",
    "        # → LSTM Layer     →     outputs, (hidden, cell)\n",
    "        self.embeding = nn.Embedding(input_dim,emb_dim) #A dictionary that turns word indices into vectors. Random at first. We will train it\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim) #Creates an LSTM layer that will handle the embedding sequence.\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embeding(src)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return hidden, cell"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
