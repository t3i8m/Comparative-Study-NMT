{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\cecilia\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\cecilia\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\cecilia\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\cecilia\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\cecilia\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\cecilia\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Cecilia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
    "nltk.download('punkt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eng -> german sentence pairs\n",
    "pairs = [\n",
    "    [\"i am a student\", \"ich bin ein student\"],\n",
    "    [\"he is a teacher\", \"er ist ein lehrer\"],\n",
    "    [\"she is happy\", \"sie ist glücklich\"],\n",
    "    [\"they are doctors\", \"sie sind ärzte\"],\n",
    "    [\"we are friends\", \"wir sind freunde\"],\n",
    "    [\"i am tired\", \"ich bin müde\"],\n",
    "    [\"you are smart\", \"du bist klug\"],\n",
    "    [\"it is raining\", \"es regnet\"],\n",
    "    [\"i like apples\", \"ich mag äpfel\"],\n",
    "    [\"he likes music\", \"er mag musik\"],\n",
    "    [\"do you speak german\", \"sprichst du deutsch\"],\n",
    "    [\"what is your name\", \"wie heißt du\"],\n",
    "    [\"my name is anna\", \"ich heiße anna\"],\n",
    "    [\"i live in berlin\", \"ich wohne in berlin\"],\n",
    "    [\"she lives in hamburg\", \"sie wohnt in hamburg\"],\n",
    "    [\"i am hungry\", \"ich habe hunger\"],\n",
    "    [\"are you okay\", \"bist du okay\"],\n",
    "    [\"this is my book\", \"das ist mein buch\"],\n",
    "    [\"i need help\", \"ich brauche hilfe\"],\n",
    "    [\"let's go\", \"lass uns gehen\"],\n",
    "    [\"hi i am a teacher\", \"hallo ich bin ein lehrer\"],\n",
    "    [\"hello i am a teacher\", \"hallo ich bin ein lehrer\"],\n",
    "    [\"i am a teacher\", \"ich bin ein lehrer\"]\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence:str)->(list):\n",
    "    return sentence.lower().split() \n",
    "\n",
    "def build_vocab(sentences:list)->(dict):\n",
    "    # building a vocabullary so each word had an index\n",
    "    # <pad> - to align sentences\n",
    "    # <sos> - \"start of sentence\" (inserted before each sentence)\n",
    "    # <eos> - \"end of sentence\" (inserted at the end)\n",
    "    vocab = {\"<pad>\":0, \"<sos>\":1, \"<eos>\":2,}\n",
    "    idx = 3 # starting point\n",
    "    for sentence in sentences:\n",
    "        for word in tokenize(sentence):\n",
    "            if word not in vocab:\n",
    "                vocab[word] = idx\n",
    "                idx+=1\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, '<sos>': 1, '<eos>': 2, 'i': 3, 'am': 4, 'a': 5, 'student': 6, 'he': 7, 'is': 8, 'teacher': 9, 'she': 10, 'happy': 11, 'they': 12, 'are': 13, 'doctors': 14, 'we': 15, 'friends': 16, 'tired': 17, 'you': 18, 'smart': 19, 'it': 20, 'raining': 21, 'like': 22, 'apples': 23, 'likes': 24, 'music': 25, 'do': 26, 'speak': 27, 'german': 28, 'what': 29, 'your': 30, 'name': 31, 'my': 32, 'anna': 33, 'live': 34, 'in': 35, 'berlin': 36, 'lives': 37, 'hamburg': 38, 'hungry': 39, 'okay': 40, 'this': 41, 'book': 42, 'need': 43, 'help': 44, \"let's\": 45, 'go': 46, 'hi': 47, 'hello': 48}\n",
      "{'<pad>': 0, '<sos>': 1, '<eos>': 2, 'ich': 3, 'bin': 4, 'ein': 5, 'student': 6, 'er': 7, 'ist': 8, 'lehrer': 9, 'sie': 10, 'glücklich': 11, 'sind': 12, 'ärzte': 13, 'wir': 14, 'freunde': 15, 'müde': 16, 'du': 17, 'bist': 18, 'klug': 19, 'es': 20, 'regnet': 21, 'mag': 22, 'äpfel': 23, 'musik': 24, 'sprichst': 25, 'deutsch': 26, 'wie': 27, 'heißt': 28, 'heiße': 29, 'anna': 30, 'wohne': 31, 'in': 32, 'berlin': 33, 'wohnt': 34, 'hamburg': 35, 'habe': 36, 'hunger': 37, 'okay': 38, 'das': 39, 'mein': 40, 'buch': 41, 'brauche': 42, 'hilfe': 43, 'lass': 44, 'uns': 45, 'gehen': 46, 'hallo': 47}\n"
     ]
    }
   ],
   "source": [
    "# creating eng and ger vocabs\n",
    "eng_vocab = build_vocab([n[0] for n in pairs])\n",
    "german_vocab = build_vocab([n[1] for n in pairs])\n",
    "print(eng_vocab)\n",
    "print(german_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 3, 4, 5, 6, 2],\n",
       " [1, 7, 8, 5, 9, 2],\n",
       " [1, 10, 8, 11, 2],\n",
       " [1, 12, 13, 14, 2],\n",
       " [1, 15, 13, 16, 2],\n",
       " [1, 3, 4, 17, 2],\n",
       " [1, 18, 13, 19, 2],\n",
       " [1, 20, 8, 21, 2],\n",
       " [1, 3, 22, 23, 2],\n",
       " [1, 7, 24, 25, 2],\n",
       " [1, 26, 18, 27, 28, 2],\n",
       " [1, 29, 8, 30, 31, 2],\n",
       " [1, 32, 31, 8, 33, 2],\n",
       " [1, 3, 34, 35, 36, 2],\n",
       " [1, 10, 37, 35, 38, 2],\n",
       " [1, 3, 4, 39, 2],\n",
       " [1, 13, 18, 40, 2],\n",
       " [1, 41, 8, 32, 42, 2],\n",
       " [1, 3, 43, 44, 2],\n",
       " [1, 45, 46, 2],\n",
       " [1, 47, 3, 4, 5, 9, 2],\n",
       " [1, 48, 3, 4, 5, 9, 2],\n",
       " [1, 3, 4, 5, 9, 2]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sentence_to_indices(sentence:str, vocab:dict)->(list):\n",
    "    tokens = tokenize(sentence)\n",
    "    return [vocab[\"<sos>\"]]+[vocab[n] for n in tokens]+[vocab[\"<eos>\"]]\n",
    "\n",
    "eng_indices = [sentence_to_indices(n[0], eng_vocab) for n in pairs]\n",
    "eng_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch(pairs:list, eng_vocab:dict, german_vocab:dict):\n",
    "    src_batch = [] # eng\n",
    "    trg_batch = [] #german\n",
    "\n",
    "    for eng, ger in pairs:\n",
    "        src = sentence_to_indices(eng, eng_vocab)\n",
    "        trg = sentence_to_indices(ger, german_vocab)\n",
    "\n",
    "        # torch.tensor(src, dtype=torch.long) -> converting to the tensors (arrays)\n",
    "        src_batch.append(torch.tensor(src, dtype=torch.long))\n",
    "        trg_batch.append(torch.tensor(trg, dtype=torch.long))\n",
    "\n",
    "    # src_batch = [\n",
    "        # tensor([1, 3, 4, 2]),        # \"i am happy\"\n",
    "        # tensor([1, 3, 4, 5, 6, 2])   # \"i am a student\"\n",
    "        # ]\n",
    "    # pad_sequence(src_batch, padding_value=0) →\n",
    "        # tensor([\n",
    "        # [1, 1],\n",
    "        # [3, 3],\n",
    "        # [4, 4],\n",
    "        # [2, 5],\n",
    "        # [0, 6],\n",
    "        # [0, 2]\n",
    "        # ])  so, the first column is the first sentence and the second is the second the zeros are the paddings, so we had vectors of the same lengths\n",
    "\n",
    "    src_batch = nn.utils.rnn.pad_sequence(src_batch, padding_value=0)\n",
    "    trg_batch = nn.utils.rnn.pad_sequence(trg_batch, padding_value=0)\n",
    "\n",
    "    return src_batch, trg_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1],\n",
       "        [ 3,  7, 10, 12, 15,  3, 18, 20,  3,  7, 26, 29, 32,  3, 10,  3, 13, 41,\n",
       "          3, 45, 47, 48,  3],\n",
       "        [ 4,  8,  8, 13, 13,  4, 13,  8, 22, 24, 18,  8, 31, 34, 37,  4, 18,  8,\n",
       "         43, 46,  3,  3,  4],\n",
       "        [ 5,  5, 11, 14, 16, 17, 19, 21, 23, 25, 27, 30,  8, 35, 35, 39, 40, 32,\n",
       "         44,  2,  4,  4,  5],\n",
       "        [ 6,  9,  2,  2,  2,  2,  2,  2,  2,  2, 28, 31, 33, 36, 38,  2,  2, 42,\n",
       "          2,  0,  5,  5,  9],\n",
       "        [ 2,  2,  0,  0,  0,  0,  0,  0,  0,  0,  2,  2,  2,  2,  2,  0,  0,  2,\n",
       "          0,  0,  9,  9,  2],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  2,  2,  0]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_batch, trg_batch = prepare_batch(pairs, eng_vocab, german_vocab)\n",
    "# getting the batches and send them to the device\n",
    "src_batch=src_batch.to(device)\n",
    "trg_batch=trg_batch.to(device)\n",
    "src_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        # input_dim - how many words are in the dictionary (dictionary size)\n",
    "        # emb_dim - size of each embedding (vector length, e.g. 32 or 100)\n",
    "        # hidden_dim - how many neurons inside the LSTM, the size of the \"memory\"\n",
    "\n",
    "        # Example:\n",
    "        # The word \"student\" has an embedding index of 42\n",
    "        # embedding(42) produces a vector of sorts: [0.12, -0.03, 0.44, ..., 0.08] of length emb_dim\n",
    "\n",
    "        # Input (indices):       [1, 4, 5, 2]\n",
    "        # → Embedding Layer →    [[...], [...], [...], [...]]  # (seq_len, emb_dim)\n",
    "        # → LSTM Layer     →     outputs, (hidden, cell)\n",
    "        self.embeding = nn.Embedding(input_dim,emb_dim) #A dictionary that turns word indices into vectors. Random at first. We will train it\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim) #Creates an LSTM layer that will handle the embedding sequence.\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embeding(src)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        # output_dim - output dictionary size (how many words in the target language, e.g. German)\n",
    "        # emb_dim - embbedding size (length of each word vector)\n",
    "        # hidden_dim — how many neurons are in the LSTM (hidden state size)\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim) # (output_dim x emb_dim) matrix\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim) # (output, hidden, cell)\n",
    "        # output\n",
    "        # This is all the hidden states (h_t) at each step of the sequence.\n",
    "        # Size: (seq_len, batch_size, hidden_dim)\n",
    "        # That is: for each word → corresponding h_t\n",
    "\n",
    "        # hidden - shortterm, is what the encoder \"understood\" about the entire sequence.\n",
    "        # This last hidden state (h_T) is the final \"memory state\".\n",
    "        # Size: (num_layers * num_directions, batch_size, hidden_dim)\n",
    "        # Normally num_layers = 1, num_directions = 1, so:\n",
    "        # hidden = (1, batch_size, hidden_dim)\n",
    "\n",
    "        # cell - longterm \n",
    "        # This is the last state of the \"memory cell\" (c_T)\n",
    "        # Similar to hidden but stores deep state, used for internal LSTM memory.\n",
    "            \n",
    "        # Example:\n",
    "        # Output hidden from LSTM → [0.5, -0.2, 1.1, ...] (size hidden_dim)\n",
    "        # fc_out turns this into a vector of length output_dim\n",
    "        # Then do softmax → probabilities of all words\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim) #Converts the hidden state (hidden) to a probability vector of all words\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        input = input.unsqueeze(0) # we get (1, batch_size)\n",
    "        embedded = self.embedding(input)\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.fc_out.out_features\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(device)\n",
    "\n",
    "        hidden, cell = self.encoder(src)\n",
    "        input = trg[0, :]  # <sos>\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bleu(model, pairs, eng_vocab, german_vocab):\n",
    "    smooth = SmoothingFunction().method4\n",
    "    bleu_scores = []\n",
    "\n",
    "    for eng, ger in pairs:\n",
    "        pred = translate_sentence(eng, model, eng_vocab, german_vocab)\n",
    "        reference = tokenize(ger)\n",
    "        candidate = tokenize(pred)\n",
    "        score = sentence_bleu([reference], candidate, smoothing_function=smooth)\n",
    "        bleu_scores.append(score)\n",
    "\n",
    "    average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "    return average_bleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_corpus_bleu(model, pairs, eng_vocab, german_vocab):\n",
    "    list_of_references = []\n",
    "    hypotheses = []\n",
    "\n",
    "    for eng, ger in pairs:\n",
    "        pred = translate_sentence(eng, model, eng_vocab, german_vocab)\n",
    "        reference = nltk.word_tokenize(ger.lower())\n",
    "        candidate = nltk.word_tokenize(pred.lower())\n",
    "\n",
    "        list_of_references.append([reference])\n",
    "        hypotheses.append(candidate)\n",
    "\n",
    "    score = corpus_bleu(list_of_references, hypotheses)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss: 3.8428 | BLEU: 0.0017\n",
      "Epoch 10 | Loss: 3.7765 | BLEU: 0.0186\n",
      "Epoch 20 | Loss: 3.7236 | BLEU: 0.0364\n",
      "Epoch 30 | Loss: 3.6279 | BLEU: 0.0375\n",
      "Epoch 40 | Loss: 3.4385 | BLEU: 0.0375\n",
      "Epoch 50 | Loss: 3.1822 | BLEU: 0.0375\n",
      "Epoch 60 | Loss: 2.9710 | BLEU: 0.0375\n",
      "Epoch 70 | Loss: 2.8500 | BLEU: 0.0323\n",
      "Epoch 80 | Loss: 2.6970 | BLEU: 0.0323\n",
      "Epoch 90 | Loss: 2.5602 | BLEU: 0.0295\n",
      "Epoch 100 | Loss: 2.4380 | BLEU: 0.0450\n",
      "Epoch 110 | Loss: 2.3118 | BLEU: 0.0851\n",
      "Epoch 120 | Loss: 2.2025 | BLEU: 0.0797\n",
      "Epoch 130 | Loss: 2.0568 | BLEU: 0.0893\n",
      "Epoch 140 | Loss: 1.9832 | BLEU: 0.1331\n",
      "Epoch 150 | Loss: 1.8659 | BLEU: 0.1682\n",
      "Epoch 160 | Loss: 1.7199 | BLEU: 0.1772\n",
      "Epoch 170 | Loss: 1.6043 | BLEU: 0.1788\n",
      "Epoch 180 | Loss: 1.5748 | BLEU: 0.1803\n",
      "Epoch 190 | Loss: 1.4012 | BLEU: 0.2112\n",
      "Epoch 200 | Loss: 1.3343 | BLEU: 0.2320\n",
      "Epoch 210 | Loss: 1.2088 | BLEU: 0.2289\n",
      "Epoch 220 | Loss: 1.1090 | BLEU: 0.2608\n",
      "Epoch 230 | Loss: 1.0128 | BLEU: 0.2676\n",
      "Epoch 240 | Loss: 0.9298 | BLEU: 0.2832\n",
      "Epoch 250 | Loss: 0.8501 | BLEU: 0.3645\n",
      "Epoch 260 | Loss: 0.7738 | BLEU: 0.3862\n",
      "Epoch 270 | Loss: 0.6984 | BLEU: 0.4606\n",
      "Epoch 280 | Loss: 0.6341 | BLEU: 0.4844\n",
      "Epoch 290 | Loss: 0.5771 | BLEU: 0.5036\n",
      "Epoch 300 | Loss: 0.5246 | BLEU: 0.5194\n",
      "Epoch 310 | Loss: 0.4762 | BLEU: 0.5194\n",
      "Epoch 320 | Loss: 0.4352 | BLEU: 0.5482\n",
      "Epoch 330 | Loss: 0.3968 | BLEU: 0.5892\n",
      "Epoch 340 | Loss: 0.3647 | BLEU: 0.5924\n",
      "Epoch 350 | Loss: 0.3351 | BLEU: 0.5924\n",
      "Epoch 360 | Loss: 0.3077 | BLEU: 0.5924\n",
      "Epoch 370 | Loss: 0.2848 | BLEU: 0.5924\n",
      "Epoch 380 | Loss: 0.2633 | BLEU: 0.5924\n",
      "Epoch 390 | Loss: 0.2440 | BLEU: 0.5924\n",
      "Epoch 400 | Loss: 0.2265 | BLEU: 0.5924\n",
      "Epoch 410 | Loss: 0.2107 | BLEU: 0.5924\n",
      "Epoch 420 | Loss: 0.1965 | BLEU: 0.5924\n",
      "Epoch 430 | Loss: 0.1836 | BLEU: 0.6171\n",
      "Epoch 440 | Loss: 0.1719 | BLEU: 0.6428\n",
      "Epoch 450 | Loss: 0.1613 | BLEU: 0.6428\n",
      "Epoch 460 | Loss: 0.1517 | BLEU: 0.6428\n",
      "Epoch 470 | Loss: 0.1429 | BLEU: 0.6428\n",
      "Epoch 480 | Loss: 0.1349 | BLEU: 0.6428\n",
      "Epoch 490 | Loss: 0.1275 | BLEU: 0.6428\n",
      "Epoch 500 | Loss: 0.1208 | BLEU: 0.6428\n",
      "Epoch 510 | Loss: 0.1145 | BLEU: 0.6428\n",
      "Epoch 520 | Loss: 0.1088 | BLEU: 0.6428\n",
      "Epoch 530 | Loss: 0.1035 | BLEU: 0.6433\n",
      "Epoch 540 | Loss: 0.0986 | BLEU: 0.6433\n",
      "Epoch 550 | Loss: 0.0940 | BLEU: 0.6433\n",
      "Epoch 560 | Loss: 0.0898 | BLEU: 0.6433\n",
      "Epoch 570 | Loss: 0.0858 | BLEU: 0.6433\n",
      "Epoch 580 | Loss: 0.0821 | BLEU: 0.6433\n",
      "Epoch 590 | Loss: 0.0787 | BLEU: 0.6433\n",
      "Epoch 600 | Loss: 0.0754 | BLEU: 0.6433\n",
      "Epoch 610 | Loss: 0.0724 | BLEU: 0.6433\n",
      "Epoch 620 | Loss: 0.0696 | BLEU: 0.6433\n",
      "Epoch 630 | Loss: 0.0669 | BLEU: 0.6433\n",
      "Epoch 640 | Loss: 0.0644 | BLEU: 0.6433\n",
      "Epoch 650 | Loss: 0.0620 | BLEU: 0.6433\n",
      "Epoch 660 | Loss: 0.0598 | BLEU: 0.6433\n",
      "Epoch 670 | Loss: 0.0577 | BLEU: 0.6433\n",
      "Epoch 680 | Loss: 0.0557 | BLEU: 0.6433\n",
      "Epoch 690 | Loss: 0.0538 | BLEU: 0.6433\n",
      "Epoch 700 | Loss: 0.0520 | BLEU: 0.6433\n",
      "Epoch 710 | Loss: 0.0503 | BLEU: 0.6433\n",
      "Epoch 720 | Loss: 0.0487 | BLEU: 0.6433\n",
      "Epoch 730 | Loss: 0.0471 | BLEU: 0.6433\n",
      "Epoch 740 | Loss: 0.0457 | BLEU: 0.6433\n",
      "Epoch 750 | Loss: 0.0443 | BLEU: 0.6433\n",
      "Epoch 760 | Loss: 0.0430 | BLEU: 0.6433\n",
      "Epoch 770 | Loss: 0.0417 | BLEU: 0.6433\n",
      "Epoch 780 | Loss: 0.0405 | BLEU: 0.6433\n",
      "Epoch 790 | Loss: 0.0394 | BLEU: 0.6433\n",
      "Epoch 800 | Loss: 0.0383 | BLEU: 0.6433\n",
      "Epoch 810 | Loss: 0.0372 | BLEU: 0.6433\n",
      "Epoch 820 | Loss: 0.0362 | BLEU: 0.6433\n",
      "Epoch 830 | Loss: 0.0352 | BLEU: 0.6433\n",
      "Epoch 840 | Loss: 0.0343 | BLEU: 0.6433\n",
      "Epoch 850 | Loss: 0.0334 | BLEU: 0.6433\n",
      "Epoch 860 | Loss: 0.0326 | BLEU: 0.6433\n",
      "Epoch 870 | Loss: 0.0318 | BLEU: 0.6433\n",
      "Epoch 880 | Loss: 0.0310 | BLEU: 0.6433\n",
      "Epoch 890 | Loss: 0.0302 | BLEU: 0.6433\n",
      "Epoch 900 | Loss: 0.0295 | BLEU: 0.6433\n",
      "Epoch 910 | Loss: 0.0288 | BLEU: 0.6433\n",
      "Epoch 920 | Loss: 0.0282 | BLEU: 0.6433\n",
      "Epoch 930 | Loss: 0.0275 | BLEU: 0.6433\n",
      "Epoch 940 | Loss: 0.0269 | BLEU: 0.6433\n",
      "Epoch 950 | Loss: 0.0263 | BLEU: 0.6433\n",
      "Epoch 960 | Loss: 0.0257 | BLEU: 0.6433\n",
      "Epoch 970 | Loss: 0.0251 | BLEU: 0.6433\n",
      "Epoch 980 | Loss: 0.0246 | BLEU: 0.6433\n",
      "Epoch 990 | Loss: 0.0241 | BLEU: 0.6433\n",
      "\n",
      "Final Corpus BLEU Score: 0.7259\n",
      "Final Corpus BLEU (%): 72.59%\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(eng_vocab)\n",
    "OUTPUT_DIM = len(german_vocab)\n",
    "EMB_DIM = 16\n",
    "HID_DIM = 32\n",
    "\n",
    "encoder = Encoder(INPUT_DIM, EMB_DIM, HID_DIM).to(device)\n",
    "decoder = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM).to(device)\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "def train(model, src, trg, optimizer, criterion):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(src, trg)\n",
    "    output_dim = output.shape[-1]\n",
    "    output = output[1:].view(-1, output_dim)\n",
    "    trg = trg[1:].view(-1)\n",
    "    loss = criterion(output, trg)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1000):\n",
    "    loss = train(model, src_batch, trg_batch, optimizer, criterion)\n",
    "    if epoch % 10 == 0:\n",
    "        bleu = evaluate_bleu(model, pairs, eng_vocab, german_vocab)\n",
    "        print(f\"Epoch {epoch} | Loss: {loss:.4f} | BLEU: {bleu:.4f}\")\n",
    "\n",
    "# Final evaluation with corpus BLEU\n",
    "final_corpus_bleu = evaluate_corpus_bleu(model, pairs, eng_vocab, german_vocab)\n",
    "print(f\"\\nFinal Corpus BLEU Score: {final_corpus_bleu:.4f}\")\n",
    "print(f\"Final Corpus BLEU (%): {final_corpus_bleu * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, model, eng_vocab, german_vocab, max_len=20):\n",
    "    model.eval()\n",
    "\n",
    "    idx2word = {idx: word for word, idx in german_vocab.items()}\n",
    "\n",
    "    # tokenization\n",
    "    tokens = [\"<sos>\"] + tokenize(sentence.lower()) + [\"<eos>\"]\n",
    "    src_indices = [eng_vocab.get(token, eng_vocab[\"<pad>\"]) for token in tokens]\n",
    "    src_tensor = torch.tensor(src_indices, dtype=torch.long).unsqueeze(1).to(device)  # (seq_len, 1)\n",
    "\n",
    "    # Пgo through the encoder\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(src_tensor)\n",
    "\n",
    "    # decoding with <sos>\n",
    "    trg_indices = [german_vocab[\"<sos>\"]]\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        trg_tensor = torch.tensor([trg_indices[-1]], dtype=torch.long).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n",
    "            pred_token = output.argmax(1).item()\n",
    "\n",
    "        if pred_token == german_vocab[\"<eos>\"]:\n",
    "            break\n",
    "\n",
    "        trg_indices.append(pred_token)\n",
    "\n",
    "    translated_words = [idx2word[idx] for idx in trg_indices[1:]]  # without <sos>\n",
    "    return \" \".join(translated_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hallo ich bin ein lehrer'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_sentence(\"hi i am a teacher\",model, eng_vocab, german_vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
