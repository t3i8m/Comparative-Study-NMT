{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-18T13:06:03.383610200Z",
     "start_time": "2025-04-18T13:06:03.350543300Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install -U datasets\n",
    "#!pip install -U sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import re\n",
    "import unicodedata"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-18T13:06:03.409619900Z",
     "start_time": "2025-04-18T13:06:03.392506900Z"
    }
   },
   "id": "2cc3a67ce2fcd308",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"wmt14\", \"de-en\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-18T13:06:19.364775800Z",
     "start_time": "2025-04-18T13:06:03.406422600Z"
    }
   },
   "id": "b4c8bcc78150a21e",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 4508785\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3003\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-18T13:06:19.384810900Z",
     "start_time": "2025-04-18T13:06:19.374590100Z"
    }
   },
   "id": "f599e30b35222572",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/4508785 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "38a41dde7ca64c188b163f98422b1081"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Filter:   0%|          | 0/4508785 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7752e94e193246ef889a0a85b921843d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "45dfa4e7b91547199022b61a319b517c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Filter:   0%|          | 0/3000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ec686ddfae5e435bb2f8d6f994442e91"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/3003 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fb81973f195e4cf2aebaaeadf622437e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Filter:   0%|          | 0/3003 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7e3942a329ae4f6b9c44cec735d017ab"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check if examples are not empty\n",
    "def is_valid(example):\n",
    "    return example[\"translation\"][\"en\"].strip() != \"\" and example[\"translation\"][\"de\"].strip() != \"\"\n",
    "\n",
    "# Remove commas from inside numbers \n",
    "def remove_number_commas(text):\n",
    "    return re.sub(r'(?<=\\d),(?=\\d)', '', text)\n",
    "\n",
    "# Space out punctuation \n",
    "def space_out_punctuation(text):\n",
    "    return re.sub(r'([^\\w\\s])', r' \\1 ', text)\n",
    "\n",
    "# Remove control characters\n",
    "def remove_control_chars(text):\n",
    "    return re.sub(r'[\\x00-\\x1F\\x7F]', '', text)\n",
    "\n",
    "# Normalize unicode\n",
    "def normalize_unicode(text):\n",
    "    return unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "#clean data\n",
    "def clean_text(example):\n",
    "    for lang in [\"en\", \"de\"]:\n",
    "        text = example[\"translation\"][lang]\n",
    "        text = normalize_unicode(text)\n",
    "        text = remove_control_chars(text)\n",
    "        text = remove_number_commas(text)\n",
    "        text = space_out_punctuation(text)\n",
    "        text = text.strip()\n",
    "        example[\"translation\"][lang] = text\n",
    "    return example\n",
    "\n",
    "# Remove duplicates\n",
    "def is_not_duplicate(example):\n",
    "    return example[\"translation\"][\"en\"] != example[\"translation\"][\"de\"]\n",
    "\n",
    "# Apply cleaning\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    dataset[split] = dataset[split].filter(is_valid)\n",
    "    dataset[split] = dataset[split].map(clean_text)\n",
    "    dataset[split] = dataset[split].filter(is_not_duplicate)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-18T13:24:38.987287200Z",
     "start_time": "2025-04-18T13:06:19.403938800Z"
    }
   },
   "id": "6f81698fc078a8b9",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Save English and German translations to separate files\n",
    "def save_translation_files(dataset_split, split=\"train\"):\n",
    "    with open(f\"{split}.en\", \"w\", encoding=\"utf-8\") as f_en, \\\n",
    "            open(f\"{split}.de\", \"w\", encoding=\"utf-8\") as f_de:\n",
    "        for example in dataset_split:\n",
    "            f_en.write(example[\"translation\"][\"en\"] + \"\\n\")\n",
    "            f_de.write(example[\"translation\"][\"de\"] + \"\\n\")\n",
    "\n",
    "save_translation_files(dataset[\"train\"], split=\"train\")\n",
    "save_translation_files(dataset[\"validation\"], split=\"validation\")\n",
    "save_translation_files(dataset[\"test\"], split=\"test\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-18T13:33:05.691606500Z",
     "start_time": "2025-04-18T13:24:38.990508100Z"
    }
   },
   "id": "b92b1621160fcf8a",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Train a joint BPE tokenizer on both English and German data\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=\"train.en,train.de\",\n",
    "    model_prefix=\"bpe_joint\",\n",
    "    model_type=\"bpe\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-18T13:39:33.624325200Z",
     "start_time": "2025-04-18T13:33:05.700793800Z"
    }
   },
   "id": "23d255f9bdf4e914",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def tokenize_and_save(dataset_split, split=\"train\", src_lang=\"en\", tgt_lang=\"de\", limit=None):\n",
    "    with open(f\"{split}.src\", \"w\", encoding=\"utf-8\") as f_src, \\\n",
    "            open(f\"{split}.tgt\", \"w\", encoding=\"utf-8\") as f_tgt:\n",
    "        for i, example in enumerate(dataset_split):\n",
    "            if limit is not None and i >= limit:\n",
    "                break\n",
    "            f_src.write(\" \".join(tokenizer.encode(example[\"translation\"][src_lang], out_type=str)) + \"\\n\")\n",
    "            f_tgt.write(\" \".join(tokenizer.encode(example[\"translation\"][tgt_lang], out_type=str)) + \"\\n\")\n",
    "\n",
    "tokenize_and_save(dataset[\"train\"], split=\"train\", limit=None)\n",
    "tokenize_and_save(dataset[\"validation\"], split=\"validation\", limit=None)\n",
    "tokenize_and_save(dataset[\"test\"], split=\"test\", limit=None)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-18T14:07:38.145737300Z",
     "start_time": "2025-04-18T13:39:33.675317300Z"
    }
   },
   "id": "2bdbc37ce2805a7c",
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
