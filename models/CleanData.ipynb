{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-16T12:54:09.583025800Z",
     "start_time": "2025-04-16T12:54:09.569373300Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install -U datasets\n",
    "#!pip install -U sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import re"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-16T12:54:09.619609Z",
     "start_time": "2025-04-16T12:54:09.588988900Z"
    }
   },
   "id": "2cc3a67ce2fcd308",
   "execution_count": 89
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"wmt14\", \"de-en\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-16T12:54:19.867478700Z",
     "start_time": "2025-04-16T12:54:09.598692800Z"
    }
   },
   "id": "b4c8bcc78150a21e",
   "execution_count": 90
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 4508785\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3003\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-16T12:54:19.887798Z",
     "start_time": "2025-04-16T12:54:19.871476100Z"
    }
   },
   "id": "f599e30b35222572",
   "execution_count": 91
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Remove empty examples\n",
    "def is_valid(example):\n",
    "    return example[\"translation\"][\"en\"].strip() != \"\" and example[\"translation\"][\"de\"].strip() != \"\"\n",
    "\n",
    "dataset[\"train\"] = dataset[\"train\"].filter(is_valid)\n",
    "dataset[\"validation\"] = dataset[\"validation\"].filter(is_valid)\n",
    "dataset[\"test\"] = dataset[\"test\"].filter(is_valid)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-16T12:54:20.590036700Z",
     "start_time": "2025-04-16T12:54:19.877237300Z"
    }
   },
   "id": "d4a9a0b3ae7234e1",
   "execution_count": 92
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#REMOVE DUPLICATES\n",
    "def remove_duplicates(example):\n",
    "    return example[\"translation\"][\"en\"] != example[\"translation\"][\"de\"]\n",
    "dataset[\"train\"] = dataset[\"train\"].filter(remove_duplicates)\n",
    "dataset[\"validation\"] = dataset[\"validation\"].filter(remove_duplicates)\n",
    "dataset[\"test\"] = dataset[\"test\"].filter(remove_duplicates)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-16T12:54:21.308681700Z",
     "start_time": "2025-04-16T12:54:20.595035Z"
    }
   },
   "id": "c56d73be25557125",
   "execution_count": 93
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Remove commas from numbers\n",
    "def remove_number_commas(text):\n",
    "    return re.sub(r'(?<=\\d),(?=\\d)', '', text)\n",
    "\n",
    "# Space out punctuation from text\n",
    "def space_out_punctuation(text):\n",
    "    return re.sub(r'([.,!?()])', r' \\1 ', text)\n",
    "\n",
    "# Apply text cleaning functions to the dataset\n",
    "def clean_text(example):\n",
    "    example[\"translation\"][\"en\"] = remove_number_commas(example[\"translation\"][\"en\"])\n",
    "    example[\"translation\"][\"en\"] = space_out_punctuation(example[\"translation\"][\"en\"])\n",
    "\n",
    "    example[\"translation\"][\"de\"] = remove_number_commas(example[\"translation\"][\"de\"])\n",
    "    example[\"translation\"][\"de\"] = space_out_punctuation(example[\"translation\"][\"de\"])\n",
    "    return example\n",
    "\n",
    "# Apply the cleaning function\n",
    "dataset[\"train\"] = dataset[\"train\"].map(clean_text)\n",
    "dataset[\"validation\"] = dataset[\"validation\"].map(clean_text)\n",
    "dataset[\"test\"] = dataset[\"test\"].map(clean_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-16T12:54:23.220781200Z",
     "start_time": "2025-04-16T12:54:21.314995800Z"
    }
   },
   "id": "6f81698fc078a8b9",
   "execution_count": 94
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Save English and German translations to separate files\n",
    "def save_translation_files(dataset_split, split=\"train\"):\n",
    "    with open(f\"{split}.en\", \"w\", encoding=\"utf-8\") as f_en, \\\n",
    "            open(f\"{split}.de\", \"w\", encoding=\"utf-8\") as f_de:\n",
    "        for example in dataset_split:\n",
    "            f_en.write(example[\"translation\"][\"en\"] + \"\\n\")\n",
    "            f_de.write(example[\"translation\"][\"de\"] + \"\\n\")\n",
    "\n",
    "# Save the files for train, validation, and test splits\n",
    "save_translation_files(dataset[\"train\"], split=\"train\")\n",
    "save_translation_files(dataset[\"validation\"], split=\"validation\")\n",
    "save_translation_files(dataset[\"test\"], split=\"test\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-16T12:58:58.597053800Z",
     "start_time": "2025-04-16T12:54:23.224807700Z"
    }
   },
   "id": "b92b1621160fcf8a",
   "execution_count": 95
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Train a joint BPE tokenizer on both English and German data\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=\"train.en,train.de\",\n",
    "    model_prefix=\"bpe_joint\",\n",
    "    vocab_size=8000,\n",
    "    model_type=\"bpe\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-16T13:05:12.428860100Z",
     "start_time": "2025-04-16T13:05:12.404358700Z"
    }
   },
   "id": "23d255f9bdf4e914",
   "execution_count": 96
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Save tokenized data after training the tokenizer\n",
    "tokenizer = spm.SentencePieceProcessor(model_file=\"bpe_joint.model\")\n",
    "\n",
    "def tokenize_and_save(dataset_split, split=\"train\", src_lang=\"en\", tgt_lang=\"de\", limit=10000):\n",
    "    with open(f\"{split}.src\", \"w\", encoding=\"utf-8\") as f_src, \\\n",
    "            open(f\"{split}.tgt\", \"w\", encoding=\"utf-8\") as f_tgt:\n",
    "        for i, example in enumerate(dataset_split):\n",
    "            if i >= limit:\n",
    "                break\n",
    "            f_src.write(\" \".join(tokenizer.encode(example[\"translation\"][src_lang], out_type=str)) + \"\\n\")\n",
    "            f_tgt.write(\" \".join(tokenizer.encode(example[\"translation\"][tgt_lang], out_type=str)) + \"\\n\")\n",
    "\n",
    "tokenize_and_save(dataset[\"train\"], split=\"train\")\n",
    "tokenize_and_save(dataset[\"validation\"], split=\"validation\")\n",
    "tokenize_and_save(dataset[\"test\"], split=\"test\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-16T13:05:16.745690400Z",
     "start_time": "2025-04-16T13:05:12.464699Z"
    }
   },
   "id": "2bdbc37ce2805a7c",
   "execution_count": 97
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
