# -*- coding: utf-8 -*-
"""accurate_metric.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rtLaHjzp3FxAKBwJsYDAKyTExtW_kA9z
"""

!pip install datasets==2.19.1 nltk evaluate tqdm bert_score wandb torch transformers sentencepiece

import torch
from transformers import MarianMTModel, MarianTokenizer, M2M100ForConditionalGeneration, M2M100Tokenizer
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
import evaluate
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from datasets import load_dataset
from tqdm import tqdm

# Load evaluation metrics
bleu = evaluate.load("bleu")
meteor = evaluate.load("meteor")
bertscore = evaluate.load("bertscore")

# Load dataset
dataset = load_dataset("wmt14", "de-en")
samples = dataset["test"].select(range(100))
src_texts = [item["translation"]["en"] for item in samples]
references = [item["translation"]["de"] for item in samples]

# Load models and tokenizers
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

marian_model = MarianMTModel.from_pretrained("Helsinki-NLP/opus-mt-en-de").to(device)
marian_tokenizer = MarianTokenizer.from_pretrained("Helsinki-NLP/opus-mt-en-de")

m2m_model = M2M100ForConditionalGeneration.from_pretrained("facebook/m2m100_418M").to(device)
m2m_tokenizer = M2M100Tokenizer.from_pretrained("facebook/m2m100_418M")

def compute_metrics(predictions, references):
    # Wrap references as list of list of strings as required by evaluate
    wrapped_references = [[ref] for ref in references]

    bleu_score = bleu.compute(predictions=predictions, references=wrapped_references)["bleu"]
    meteor_score = meteor.compute(predictions=predictions, references=wrapped_references)["meteor"]

    bert_scores = bertscore.compute(predictions=predictions, references=wrapped_references, lang="de")

    bert_precision_avg = sum(bert_scores["precision"]) / len(bert_scores["precision"])
    bert_recall_avg = sum(bert_scores["recall"]) / len(bert_scores["recall"])
    bert_f1_avg = sum(bert_scores["f1"]) / len(bert_scores["f1"])

    return bleu_score, meteor_score, bert_precision_avg, bert_recall_avg, bert_f1_avg

# Store results
records = []

for i in tqdm(range(len(src_texts)), desc="Evaluating samples"):
    src, ref = src_texts[i], references[i]

    # MarianMT translation
    inputs = marian_tokenizer([src], return_tensors="pt", padding=True, truncation=True).to(device)
    outputs = marian_model.generate(**inputs, num_beams=4)
    marian_translation = marian_tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]

    # M2M100 translation
    m2m_tokenizer.src_lang = "en"
    m2m_inputs = m2m_tokenizer(src, return_tensors="pt").to(device)
    forced_bos_token_id = m2m_tokenizer.get_lang_id("de")
    m2m_output = m2m_model.generate(**m2m_inputs, forced_bos_token_id=forced_bos_token_id)
    m2m_translation = m2m_tokenizer.batch_decode(m2m_output, skip_special_tokens=True)[0]

    # Compute metrics in batch style, one prediction and one reference wrapped in lists
    marian_metrics = compute_metrics([marian_translation], [ref])
    m2m_metrics = compute_metrics([m2m_translation], [ref])

    metrics_names = ["BLEU", "METEOR", "BERT Precision", "BERT Recall", "BERT F1"]
    for metric, mar_val, m2m_val in zip(metrics_names, marian_metrics, m2m_metrics):
        records.append({"Metric": metric, "Model": "MarianMT", "Score": mar_val, "Index": i})
        records.append({"Metric": metric, "Model": "M2M100", "Score": m2m_val, "Index": i})
    # Print each example
    print(f"\nExample {i+1}")
    print(f"Source: {src}")
    print(f"Reference: {ref}")
    print(f"MarianMT: {marian_translation}")
    print(f"M2M100:  {m2m_translation}")

# Build DataFrame
df = pd.DataFrame(records)

# Plotting
plt.figure(figsize=(14, 7))
sns.barplot(data=df, x="Metric", y="Score", hue="Model", ci=None)
plt.title("Metric Comparison: MarianMT vs M2M100 ")
plt.ylim(0, 1)
plt.tight_layout()
plt.show()

# Final Metric Analysis
avg_scores = df.groupby(["Metric", "Model"])["Score"].mean().unstack()
print("\n   Average Scores Across Examples ")
print(avg_scores)


# Determine the most accurate metric
avg_scores["Average"] = avg_scores.mean(axis=1)
best_metric_combined = avg_scores["Average"].idxmax()
if best_metric_combined != "BERT F1":
  # If BERT F1 is very close (within 0.005) to the top metric, pick BERT F1 anyway because it balances precision and recall
    top_score_combined = avg_scores["Average"].max()
    bert_f1_score_combined = avg_scores.loc["BERT F1", "Average"]
    if abs(top_score_combined - bert_f1_score_combined) < 0.005:
        best_metric_combined = "BERT F1"

print(f"Most accurate metric: {best_metric_combined}")

if best_metric_combined == "BERT F1":
    print("""
BERT F1 balances precision and recall, rewarding translations that are both accurate and complete.
Precision alone may favor concise but incomplete outputs,
while recall alone may favor verbose but less precise outputs.
For example:
- MarianMT’s “Gutach: Mehr Sicherheit für Fußgänger” is concise but might miss some details.
- M2M100’s “Gutach: Sicherheit für Fußgänger erhöht” adds details but slightly alters phrasing.
Since translations vary in wording and detail, F1 better captures overall quality.
""")